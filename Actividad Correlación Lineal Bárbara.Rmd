---
title: "Actividad Correlación Lineal"
author: "Bárbara"
date: "2024-03-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


# Ejercicio 1: Define brevemente el concepto de correlación lineal. 

Se trata de una medida de regresión aplicada para conocer el grado de dependencia entre dos variables diferentes, estableciendo una correlación lineal entre ambas. Esta correlación cuantifica la intensidad (que va de 0 a 1) de ambas variables y si ambas son dependentes entre sí, por lo que al cambiar los valores de una cambiarán los valores de la otra.Podemos encontrar dos tipos de correlación, la directamente proporcional (que es cuando obtenemos un número positivo) e inversamente proporcional (que es cuando obtenemos un número negativo). 

# Ejercicio 2:  ¿Por qué decimos que la correlación lineal es una prueba de correlación paramétrica? ¿En qué se diferencian las pruebas paramétricas de las no paramétricas?

Decimos que la correlación lineal es una prueba de correlación paramétrica cuando los datos que presentan dos variables se encuentran medidas por intervalos, distribuyéndose de manera que cumplen algunas condiciones de validez para asegurar la fiabilidad de la prueba. La diferencia entre las pruebas paramétricas y las no paramétricas radica en que las segundas no se distribuyen, por lo que pueden aplicarse incluso si no cumple las condiciones de validez paramétricas y, al no presentar un orden de distribución, es difícil establecer una relación entre sus variables.

# Ejercicio 3: Calcula la correlación entre las variables almacenadas en la tabla ‘data1’. 

```{r echo=TRUE}
library(readxl)
data <- as.data.frame(read_excel("c:/correlación/data1.xls"))
data
```
``` {r echo=TRUE}
correlación <- cor(data1)
print(correlación)
```


# Ejercicio 4: Calcula los coeficientes de correlación de las variables junto con el nivel de significancia (p-value) en 1 solo gráfico. Interpreta los resultados.

``` {r}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  Cor <- abs(cor(x, y))
  txt <- paste0(prefix, format(c(Cor, 0.123456789), digits = digits)[1])
  if(missing(cex.cor)){
    cex.cor <- 0.4 / strewidth(txt)
  }
  text(0.5, 0.5, txt,
       cex = 1 + cex.cor*Cor)
}
```

```{r echo=TRUE}
pairs(data,
      upper.panel = panel.cor,
      lower.panel = panel.smooth) 
```

Estamos viendo que no existe una correlación entre ambas variables (altura y peso), no encontrando una correlación lineal.



# Ejercicio 5:  Emplea una función para obtener en una matriz de correlación lineal, IC 95% y p-value de todas las variables en el data frame ‘data’

```{r warning=FALSE}
library(correlation)
resultados <- correlation(data1)
resultados
```
Interpretación:la correlación en este caso es positiva porque da un coeficiente igual o inferior a 0.15, presentando una correlación lineal directamente proporcional. Como el p-value o nivel de significancia es mayor a 0.05, no podemos afirmar que lo que obtengamos no haya sido al azar. Además, el intervalo de confianza en todos los casos incluye al 0, por lo que no habría una correlación significativa entre ambas variables.

# Ejercicio 6: Visualiza gráficamente la correlación lineal existente entre las variables ‘longitud’ y ‘peso’.

```{r echo=TRUE}
library(ggpubr)
ggscatter(data, x = "altura", y = "peso",
          add = "reg.line", conf.int = TRUE,
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "altura (mm)", ylab = "Peso (mg)")
```

# Ejercicio 7:  Emplea la librería `corrplot()` para visualizar la correlación entre variables.

```{r include=FALSE}
install.packages("corrplot", repos = "http://cran.us.r-project.org")
```

```{r }
library(corrplot)
corrplot(cor(data))
```

# Ejercicio 8a: Crea 2 vectores: 'distancia' y 'n_piezas' para almacenarlos en un data frame.

```{r}
distancia <- c(1.1, 100.2, 90.3, 5.4, 57.5, 6.6, 34.7, 65.8, 57.9, 86.1)
n_piezas <- c(110, 2, 6, 98, 40, 94, 31, 5, 8, 10)
dist_ncuent <- data.frame(distancia, n_piezas)
knitr::kable(dist_ncuent)
```

# Ejercicio 8b: Calcula el coeficiente de correlación.

```{r, echo = TRUE}
cor.test(dist_ncuent$distancia, dist_ncuent$n_piezas)
```

```{r echo=TRUE}
correlation::correlation(dist_ncuent)
```
El coeficiente de correlación es de - 0.92.

# Ejercicio 8c:  Calcula el nivel de significancia.

El coeficiente de correlación  es de -0,92, lo que nos indica que existe una relación lineal inversa quasiperfecta ya que se encuentra próximo a -1. Los valores del intervalo de confianza del 95% muestran el intervalo (de valores) para el coeficiente de correlación [`-0.98`, `-0.71`]. Atendiendo a los valores de `p` podemos afirmar que la correlación es significativa ya que el p-value (`0.001`) es **inferior** al **nivel de significancia** `0.05`, ya que el IC es `0.95`, por lo que el nivel de significancia es de `0.05`.
Por tanto, resulta estadísticamente significativa.

# Ejercicio 8d: . Calcula el Intervalo de confianza al 95% en relación con el coeficiente de correlación.

```{r, echo = TRUE}
intervalo_de_confianza <- cor.test(dist_ncuent$distancia, dist_ncuent$n_piezas)$conf.int
print(intervalo_de_confianza)
```

El intervalo de confianza al 95% es de [-0.98, -0.70]

# Ejercicio 8e: ¿Qué intensidad y dirección presentan ambas variables?

La dirección es de -0.92, por tanto, al ser un número negativo, la dirección es inversamente proporcional entre ambas variables (cuando aumenta una variable, la otra disminuye, en la mismo proporción), y su intensidad es de 0.92, número que se acerca al número máximo que sería el 1.

# Ejercicio 8f: ¿Es significativa esta relación?

La relación es significativa, pues el valor de p-value es inferior a 0.05, concretamente, <0.001*** Estos `*`, a mayor cantidad de los mismos, significa que hay mayor probabilidad de que el resultado no sea al azar. Además, el hecho de que el p-value sea inferior a 0.05 quiere decir que hay menos del 5% de probabilidades de que el resultado sea al azar.

# Ejercicio 8g:  Resulta apropiado afirmar la correlación (o no) entre variables con un tamaño muestral tan reducido (n=10).

Sí, podría resultar apropiado afirmar esta correlación, pues al tener una correlación significativa, se entiende que si aumentase el número de muestras, el resultado no debería cambiar de manera significativa. Esto es porque, como se ha dicho, hay muy pocas probabilidades de que el resultado obtenido haya sido por azar, resultando ser una correlación lineal. No obstante, cierto es que en estadística se suelen emplear un mayor número de muestras.

# Ejercicio 9: Explícame con un ejemplo en R la diferencia entre una relación lineal y monótona entre 2 variables.

Partimos de que, en una relación lineal, cuando se realiza una modificación en una variable, se modifica la otra variable. Por tanto, en una correlación negativa, si una variable aumenta, la otra disminuye, siempre en la misma proporción, al igual que, en una correlación positiva, si una variable aumenta la otra también lo hará. Además, se puede representar con una línea recta en el gráfico.

```{r echo=TRUE}
library(ggpubr)
libretas <- 1:12
estuches <- 3*libretas + rnorm(12, mean = 0, sd = 6)
df_lineal <- data.frame(libretas, estuches)
ggplot(df_lineal, aes(x=libretas, y=estuches)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "libretas", y = "estcuhes") + 
  ggtitle("Relación lineal entre las variables libretas y estuches")
```

Mientras tanto, en una relación monótona en la que, siempre que los valores de una variable aumentan, los de la otra variable también aumentarán o disminuirán, pero no en la misma proporción (aquí vemos una diferencia con la regresión lineal). 

```{r echo=TRUE}
niños <- c(12, 20, 22, 34)
niñas <- c(8, 22, 23, 32)
df_monotona <- data.frame(niños,niñas)
ggplot(df_monotona, aes(x=niños, y=niñas)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "niños", y = "niñas") + 
  ggtitle("Relación monótona entre las variables niños y niñas")
```

# Ejercicio 10: ¿Qué tipo de prueba de correlación se aplica a las variables que experimentan una relación monótona? Expón un ejemplo en R.

Normalmente se utiliza, en vez de una correlación Pearson como sería en una relación lineal, para una relación monótona utilizamos la correlación de Spearman, aunque también se puede utilizar la correlación Kendall, siendo ambas pruebas no paramétricas que permiten medir la fuerza y dirección en la que dos variables se asocian, cuando estas no presentan una relación lineal. 

```{r echo=TRUE}
niños <- c(12, 20, 22, 34)
niñas <- c(8, 22, 23, 32)
df_monotona <- data.frame(niños,niñas)

correlation_spearman <- cor.test(df_monotona$niños, df_monotona$niñas, method = "spearman")
print(correlation_spearman)

correlation_kendall <- cor.test(df_monotona$niños, df_monotona$niñas, method = "kendall")
print(correlation_kendall)
```




